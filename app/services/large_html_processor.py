"""
å¤§å‹HTMLå¤„ç†æœåŠ¡ - ç®€åŒ–ç‰ˆ
"""

import re
import asyncio
import time
import gc
from typing import Dict, List, Tuple
from bs4 import BeautifulSoup, NavigableString, Tag


class LargeHtmlProcessor:
    """å¤§å‹HTMLå¤„ç†å™¨ - ç®€åŒ–ç‰ˆ"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¤§å‹HTMLå¤„ç†å™¨"""
        self.chunk_size = 50000  # æ¯å—5ä¸‡å­—ç¬¦
    
    def estimate_processing_time(self, html_length: int) -> Dict:
        """ä¼°ç®—å¤„ç†æ—¶é—´"""
        chunks = (html_length // self.chunk_size) + 1
        estimated_texts = html_length // 10
        estimated_unique_texts = estimated_texts // 3
        translation_batches = (estimated_unique_texts // 15) + 1
        estimated_translation_time = translation_batches * 0.3
        estimated_total_time = estimated_translation_time + chunks * 0.5
        
        return {
            "html_length": html_length,
            "estimated_chunks": chunks,
            "estimated_texts": estimated_texts,
            "estimated_unique_texts": estimated_unique_texts,
            "estimated_translation_time": round(estimated_translation_time, 1),
            "estimated_total_time": round(estimated_total_time, 1),
            "memory_usage_mb": round(html_length / 1024 / 1024 * 3, 1)
        }
    
    def split_html_into_chunks(self, html_content: str) -> List[Dict]:
        """å°†å¤§å‹HTMLåˆ†å‰²æˆå¯å¤„ç†çš„å—"""
        print(f"ğŸ“¦ å¼€å§‹åˆ†å‰²HTML ({len(html_content)} å­—ç¬¦)...")
        
        chunks = []
        current_pos = 0
        chunk_index = 0
        
        while current_pos < len(html_content):
            chunk_end = min(current_pos + self.chunk_size, len(html_content))
            
            # å°è¯•åœ¨æ ‡ç­¾è¾¹ç•Œåˆ†å‰²
            if chunk_end < len(html_content):
                tag_end = html_content.find('>', chunk_end)
                if tag_end != -1 and tag_end - chunk_end < 1000:
                    chunk_end = tag_end + 1
            
            chunk_content = html_content[current_pos:chunk_end]
            
            chunks.append({
                "index": chunk_index,
                "start_pos": current_pos,
                "end_pos": chunk_end,
                "content": chunk_content,
                "length": len(chunk_content)
            })
            
            current_pos = chunk_end
            chunk_index += 1
        
        print(f"ğŸ“¦ HTMLåˆ†å‰²å®Œæˆ: {len(chunks)} ä¸ªå—")
        return chunks
    
    async def process_large_html_with_ultimate_dom(self, html_content: str, dom_service, translation_service, from_lang: str, to_lang: str) -> Tuple[str, Dict]:
        """
        ä½¿ç”¨DOMæœåŠ¡çš„ç»ˆææ–¹æ³•å¤„ç†å¤§å‹HTML - 100%æå–å’Œ100%æ›¿æ¢
        """
        start_time = time.time()
        
        print("ğŸš€ å¯åŠ¨å¤§å‹HTMLç»ˆæDOMå¤„ç†æ¨¡å¼ - 100%æå–å’Œ100%æ›¿æ¢...")
        
        # 1. ä¼°ç®—å¤„ç†æ—¶é—´
        estimation = self.estimate_processing_time(len(html_content))
        print("ğŸ“Š å¤§å‹HTMLç»ˆæDOMå¤„ç†ä¼°ç®—:")
        for key, value in estimation.items():
            print(f"  {key}: {value}")
        
        # 2. åˆ†å‰²HTMLä¸ºå¯å¤„ç†çš„å—
        chunks = self.split_html_into_chunks(html_content)
        
        # 3. å¯¹æ¯ä¸ªå—ä½¿ç”¨DOMæœåŠ¡è¿›è¡Œ100%æå–
        print("ğŸ” ä½¿ç”¨DOMæœåŠ¡è¿›è¡Œ100%æå–...")
        all_dom_data = []
        all_chinese_texts = []
        
        for i, chunk in enumerate(chunks):
            print(f"  å¤„ç†å— {i+1}/{len(chunks)}...")
            
            # ä½¿ç”¨DOMæœåŠ¡æå–ä¸­æ–‡æ–‡æœ¬
            dom_data = dom_service.extract_all_chinese_with_dom(chunk["content"])
            
            # æ·»åŠ å—ä¿¡æ¯
            dom_data["chunk_index"] = i
            dom_data["chunk_start_pos"] = chunk["start_pos"]
            dom_data["chunk_end_pos"] = chunk["end_pos"]
            
            all_dom_data.append(dom_data)
            all_chinese_texts.extend(dom_data["chinese_texts"])
            
            # å®šæœŸæ¸…ç†å†…å­˜
            if (i + 1) % 3 == 0:
                gc.collect()
        
        print(f"âœ… DOMæå–å®Œæˆ: {len(all_chinese_texts)} ä¸ªä¸­æ–‡ç‰‡æ®µ")
        
        # 4. é«˜é€Ÿå¹¶å‘ç¿»è¯‘æ‰€æœ‰ä¸­æ–‡æ–‡æœ¬
        print("âš¡ å¼€å§‹é«˜é€Ÿå¹¶å‘ç¿»è¯‘...")
        
        translation_results = await translation_service.concurrent_batch_translate(
            all_chinese_texts,
            from_lang,
            to_lang,
            max_concurrent=15  # å¤§å‹HTMLä½¿ç”¨æ›´é«˜å¹¶å‘
        )
        
        # 5. åˆ›å»ºç¿»è¯‘æ˜ å°„è¡¨
        print("ğŸ“‹ åˆ›å»ºç¿»è¯‘æ˜ å°„è¡¨...")
        translation_map = dom_service.create_translation_map(translation_results)
        
        # 6. å¯¹æ¯ä¸ªå—ä½¿ç”¨DOMæœåŠ¡è¿›è¡Œ100%æ›¿æ¢
        print("ğŸ”„ ä½¿ç”¨DOMæœåŠ¡è¿›è¡Œ100%æ›¿æ¢...")
        translated_chunks = []
        
        for i, (chunk, dom_data) in enumerate(zip(chunks, all_dom_data)):
            print(f"  æ›¿æ¢å— {i+1}/{len(chunks)}...")
            
            # ä½¿ç”¨DOMæœåŠ¡çš„ç»ˆææ›¿æ¢æ–¹æ³•
            translated_chunk, chunk_stats = dom_service.ultimate_replace_chinese(
                chunk["content"],
                translation_map
            )
            
            translated_chunks.append(translated_chunk)
            
            # å®šæœŸæ¸…ç†å†…å­˜
            if (i + 1) % 3 == 0:
                gc.collect()
        
        # 7. åˆå¹¶æ‰€æœ‰å—
        final_html = "".join(translated_chunks)
        
        # 8. æœ€ç»ˆç»Ÿè®¡
        end_time = time.time()
        processing_time = round(end_time - start_time, 2)
        
        # è®¡ç®—æœ€ç»ˆæ›¿æ¢ç‡
        original_chinese = re.findall(r'[\u4e00-\u9fff]+', html_content)
        remaining_chinese = re.findall(r'[\u4e00-\u9fff]+', final_html)
        
        stats = {
            "processing_time": processing_time,
            "processing_mode": "ULTIMATE_DOM_100%",
            "chunks_processed": len(chunks),
            "total_texts": len(all_chinese_texts),
            "unique_texts": len(set(all_chinese_texts)),
            "translation_success": translation_results["success_count"],
            "translation_failed": translation_results["failed_count"],
            "translation_duration": translation_results["duration"],
            "original_chinese_count": len(original_chinese),
            "remaining_chinese_count": len(remaining_chinese),
            "replaced_count": len(original_chinese) - len(remaining_chinese),
            "replacement_rate": (len(original_chinese) - len(remaining_chinese)) / len(original_chinese) * 100 if original_chinese else 100,
            "memory_peak_mb": estimation["memory_usage_mb"],
            "remaining_texts": list(set(remaining_chinese))[:10]
        }
        
        print(f"ğŸ‰ å¤§å‹HTMLç»ˆæDOMå¤„ç†å®Œæˆ! è€—æ—¶: {processing_time}ç§’")
        print(f"ğŸ¯ ç»ˆææ›¿æ¢ç‡: {stats['replacement_rate']:.2f}%")
        print(f"âš¡ ç¿»è¯‘è€—æ—¶: {stats['translation_duration']}ç§’")
        print(f"ğŸš€ ç¿»è¯‘é€Ÿåº¦: {stats['unique_texts']/stats['translation_duration']:.1f} æ–‡æœ¬/ç§’")
        
        return final_html, stats


# åˆ›å»ºå…¨å±€å®ä¾‹
large_html_processor = LargeHtmlProcessor()
