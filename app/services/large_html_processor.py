"""
å¤§å‹HTMLå¤„ç†æœåŠ¡ - ä¸“é—¨å¤„ç†20ä¸‡å­—ç¬¦ä»¥ä¸Šçš„HTML
"""

import re
import asyncio
import time
from typing import Dict, List, Tuple, Generator
from bs4 import BeautifulSoup, NavigableString, Tag
import gc


class LargeHtmlProcessor:
    """å¤§å‹HTMLå¤„ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¤§å‹HTMLå¤„ç†å™¨"""
        self.chinese_pattern = r'[\u4e00-\u9fff]+'
        self.chunk_size = 50000  # æ¯å—5ä¸‡å­—ç¬¦
        self.max_text_length = 2000  # å•æ¬¡ç¿»è¯‘æœ€å¤§å­—ç¬¦æ•°
        self.batch_size = 50  # æ‰¹å¤„ç†å¤§å°
        
    def estimate_processing_time(self, html_length: int) -> Dict:
        """
        ä¼°ç®—å¤„ç†æ—¶é—´
        
        Args:
            html_length: HTMLé•¿åº¦
            
        Returns:
            å¤„ç†æ—¶é—´ä¼°ç®—
        """
        chunks = (html_length // self.chunk_size) + 1
        estimated_texts = html_length // 10  # ä¼°ç®—ä¸­æ–‡æ–‡æœ¬æ•°é‡
        estimated_unique_texts = estimated_texts // 3  # ä¼°ç®—å»é‡åæ•°é‡
        
        # ä¼°ç®—ç¿»è¯‘æ—¶é—´ï¼ˆå¹¶å‘å¤„ç†ï¼‰
        translation_batches = (estimated_unique_texts // 15) + 1
        estimated_translation_time = translation_batches * 0.3  # æ¯æ‰¹0.3ç§’
        
        # ä¼°ç®—æ€»å¤„ç†æ—¶é—´
        estimated_total_time = estimated_translation_time + chunks * 0.5  # è§£æå’Œæ›¿æ¢æ—¶é—´
        
        return {
            "html_length": html_length,
            "estimated_chunks": chunks,
            "estimated_texts": estimated_texts,
            "estimated_unique_texts": estimated_unique_texts,
            "estimated_translation_time": round(estimated_translation_time, 1),
            "estimated_total_time": round(estimated_total_time, 1),
            "memory_usage_mb": round(html_length / 1024 / 1024 * 3, 1)  # ä¼°ç®—å†…å­˜ä½¿ç”¨
        }
    
    def split_html_into_chunks(self, html_content: str) -> List[Dict]:
        """
        å°†å¤§å‹HTMLåˆ†å‰²æˆå¯å¤„ç†çš„å—
        
        Args:
            html_content: HTMLå†…å®¹
            
        Returns:
            HTMLå—åˆ—è¡¨
        """
        print(f"ğŸ“¦ å¼€å§‹åˆ†å‰²HTML ({len(html_content)} å­—ç¬¦)...")
        
        chunks = []
        current_pos = 0
        chunk_index = 0
        
        while current_pos < len(html_content):
            chunk_end = min(current_pos + self.chunk_size, len(html_content))
            
            # å°è¯•åœ¨æ ‡ç­¾è¾¹ç•Œåˆ†å‰²ï¼Œé¿å…ç ´åHTMLç»“æ„
            if chunk_end < len(html_content):
                # å‘åæŸ¥æ‰¾æ ‡ç­¾ç»“æŸä½ç½®
                tag_end = html_content.find('>', chunk_end)
                if tag_end != -1 and tag_end - chunk_end < 1000:
                    chunk_end = tag_end + 1
            
            chunk_content = html_content[current_pos:chunk_end]
            
            chunks.append({
                "index": chunk_index,
                "start_pos": current_pos,
                "end_pos": chunk_end,
                "content": chunk_content,
                "length": len(chunk_content)
            })
            
            current_pos = chunk_end
            chunk_index += 1
        
        print(f"ğŸ“¦ HTMLåˆ†å‰²å®Œæˆ: {len(chunks)} ä¸ªå—")
        return chunks
    
    def extract_chinese_from_chunk(self, chunk: Dict) -> Dict:
        """
        ä»HTMLå—ä¸­æå–ä¸­æ–‡æ–‡æœ¬
        
        Args:
            chunk: HTMLå—
            
        Returns:
            æå–ç»“æœ
        """
        try:
            # ä½¿ç”¨æ›´è½»é‡çš„è§£ææ–¹å¼
            soup = BeautifulSoup(chunk["content"], 'html.parser')
            
            chinese_texts = []
            text_positions = []
            
            # é€’å½’æå–æ–‡æœ¬
            def extract_texts(element, path=""):
                if isinstance(element, NavigableString):
                    text_content = str(element).strip()
                    if text_content:
                        chinese_matches = re.findall(self.chinese_pattern, text_content)
                        if chinese_matches:
                            chinese_texts.extend(chinese_matches)
                            text_positions.append({
                                "text": text_content,
                                "chinese_texts": chinese_matches,
                                "path": path,
                                "element": element
                            })
                
                elif isinstance(element, Tag):
                    current_path = f"{path}/{element.name}" if path else element.name
                    
                    # å¤„ç†å±æ€§ä¸­çš„ä¸­æ–‡
                    for attr_name, attr_value in element.attrs.items():
                        if isinstance(attr_value, str):
                            chinese_matches = re.findall(self.chinese_pattern, attr_value)
                            if chinese_matches:
                                chinese_texts.extend(chinese_matches)
                                text_positions.append({
                                    "text": attr_value,
                                    "chinese_texts": chinese_matches,
                                    "path": f"{current_path}@{attr_name}",
                                    "element": element,
                                    "attr_name": attr_name
                                })
                    
                    # é€’å½’å¤„ç†å­å…ƒç´ 
                    for child in element.children:
                        extract_texts(child, current_path)
            
            extract_texts(soup)
            
            result = {
                "chunk_index": chunk["index"],
                "soup": soup,
                "chinese_texts": chinese_texts,
                "text_positions": text_positions,
                "statistics": {
                    "total_chinese_segments": len(chinese_texts),
                    "unique_chinese_texts": len(set(chinese_texts)),
                    "text_nodes": len(text_positions)
                }
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ å— {chunk['index']} è§£æå¤±è´¥: {str(e)}")
            return {
                "chunk_index": chunk["index"],
                "error": str(e),
                "chinese_texts": [],
                "text_positions": [],
                "statistics": {"total_chinese_segments": 0, "unique_chinese_texts": 0, "text_nodes": 0}
            }
    
    def batch_texts_for_translation(self, all_chinese_texts: List[str]) -> List[List[str]]:
        """
        å°†æ–‡æœ¬åˆ†æ‰¹ç”¨äºç¿»è¯‘
        
        Args:
            all_chinese_texts: æ‰€æœ‰ä¸­æ–‡æ–‡æœ¬
            
        Returns:
            åˆ†æ‰¹çš„æ–‡æœ¬åˆ—è¡¨
        """
        # å»é‡å¹¶æŒ‰é•¿åº¦æ’åº
        unique_texts = list(set(all_chinese_texts))
        unique_texts.sort(key=len, reverse=True)
        
        batches = []
        current_batch = []
        current_length = 0
        
        for text in unique_texts:
            text_length = len(text)
            
            # å¦‚æœå•ä¸ªæ–‡æœ¬å¤ªé•¿ï¼Œå•ç‹¬å¤„ç†
            if text_length > self.max_text_length:
                if current_batch:
                    batches.append(current_batch)
                    current_batch = []
                    current_length = 0
                batches.append([text])
                continue
            
            # å¦‚æœåŠ å…¥å½“å‰æ–‡æœ¬ä¼šè¶…è¿‡é™åˆ¶ï¼Œå¼€å§‹æ–°æ‰¹æ¬¡
            if current_length + text_length > self.max_text_length or len(current_batch) >= self.batch_size:
                if current_batch:
                    batches.append(current_batch)
                current_batch = [text]
                current_length = text_length
            else:
                current_batch.append(text)
                current_length += text_length
        
        # æ·»åŠ æœ€åä¸€æ‰¹
        if current_batch:
            batches.append(current_batch)
        
        print(f"ğŸ“Š æ–‡æœ¬åˆ†æ‰¹å®Œæˆ: {len(unique_texts)} ä¸ªå”¯ä¸€æ–‡æœ¬åˆ†ä¸º {len(batches)} æ‰¹")
        return batches
    
    async def process_large_html(self, html_content: str, translation_service, from_lang: str, to_lang: str) -> Tuple[str, Dict]:
        """
        å¤„ç†å¤§å‹HTMLçš„ä¸»è¦æ–¹æ³•
        
        Args:
            html_content: HTMLå†…å®¹
            translation_service: ç¿»è¯‘æœåŠ¡
            from_lang: æºè¯­è¨€
            to_lang: ç›®æ ‡è¯­è¨€
            
        Returns:
            (ç¿»è¯‘åçš„HTML, ç»Ÿè®¡ä¿¡æ¯)
        """
        start_time = time.time()
        
        # 1. ä¼°ç®—å¤„ç†æ—¶é—´
        estimation = self.estimate_processing_time(len(html_content))
        print("ğŸ“Š å¤§å‹HTMLå¤„ç†ä¼°ç®—:")
        for key, value in estimation.items():
            print(f"  {key}: {value}")
        
        # 2. åˆ†å‰²HTML
        chunks = self.split_html_into_chunks(html_content)
        
        # 3. å¹¶è¡Œæå–ä¸­æ–‡æ–‡æœ¬
        print("ğŸ” å¹¶è¡Œæå–ä¸­æ–‡æ–‡æœ¬...")
        chunk_results = []
        all_chinese_texts = []
        
        for chunk in chunks:
            result = self.extract_chinese_from_chunk(chunk)
            chunk_results.append(result)
            all_chinese_texts.extend(result["chinese_texts"])
            
            # å®šæœŸæ¸…ç†å†…å­˜
            if len(chunk_results) % 5 == 0:
                gc.collect()
        
        print(f"âœ… æ–‡æœ¬æå–å®Œæˆ: {len(all_chinese_texts)} ä¸ªä¸­æ–‡ç‰‡æ®µ")
        
        # 4. åˆ†æ‰¹ç¿»è¯‘
        text_batches = self.batch_texts_for_translation(all_chinese_texts)
        
        print(f"âš¡ å¼€å§‹åˆ†æ‰¹å¹¶å‘ç¿»è¯‘ {len(text_batches)} æ‰¹...")
        
        # å¹¶å‘ç¿»è¯‘æ‰€æœ‰æ‰¹æ¬¡
        translation_tasks = []
        for batch in text_batches:
            task = translation_service.concurrent_batch_translate(
                batch, from_lang, to_lang, max_concurrent=10
            )
            translation_tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ç¿»è¯‘å®Œæˆ
        batch_results = await asyncio.gather(*translation_tasks)
        
        # 5. åˆå¹¶ç¿»è¯‘ç»“æœ
        translation_map = {}
        total_success = 0
        total_failed = 0
        
        for batch_result in batch_results:
            for translation in batch_result["translations"]:
                if translation["success"]:
                    translation_map[translation["original"]] = translation["translated"]
                    total_success += 1
                else:
                    total_failed += 1
        
        print(f"âœ… ç¿»è¯‘å®Œæˆ: æˆåŠŸ {total_success}, å¤±è´¥ {total_failed}")
        
        # 6. åˆ†å—æ›¿æ¢
        print("ğŸ”„ å¼€å§‹åˆ†å—æ›¿æ¢...")
        translated_chunks = []
        
        for i, (chunk, chunk_result) in enumerate(zip(chunks, chunk_results)):
            if "error" in chunk_result:
                translated_chunks.append(chunk["content"])
                continue
            
            # åœ¨å—ä¸­æ›¿æ¢æ–‡æœ¬
            translated_chunk = self._replace_in_chunk(chunk_result, translation_map)
            translated_chunks.append(translated_chunk)
            
            print(f"  å— {i+1}/{len(chunks)} æ›¿æ¢å®Œæˆ")
            
            # å®šæœŸæ¸…ç†å†…å­˜
            if i % 5 == 0:
                gc.collect()
        
        # 7. åˆå¹¶ç»“æœ
        final_html = "".join(translated_chunks)
        
        # 8. ç»Ÿè®¡ç»“æœ
        end_time = time.time()
        processing_time = round(end_time - start_time, 2)
        
        # è®¡ç®—æ›¿æ¢ç‡
        original_chinese = re.findall(self.chinese_pattern, html_content)
        remaining_chinese = re.findall(self.chinese_pattern, final_html)
        
        stats = {
            "processing_time": processing_time,
            "chunks_processed": len(chunks),
            "total_texts": len(all_chinese_texts),
            "unique_texts": len(set(all_chinese_texts)),
            "translation_success": total_success,
            "translation_failed": total_failed,
            "original_chinese_count": len(original_chinese),
            "remaining_chinese_count": len(remaining_chinese),
            "replaced_count": len(original_chinese) - len(remaining_chinese),
            "replacement_rate": (len(original_chinese) - len(remaining_chinese)) / len(original_chinese) * 100 if original_chinese else 100,
            "memory_peak_mb": estimation["memory_usage_mb"]
        }
        
        print(f"ğŸ‰ å¤§å‹HTMLå¤„ç†å®Œæˆ! è€—æ—¶: {processing_time}ç§’")
        
        return final_html, stats
    
    def _replace_in_chunk(self, chunk_result: Dict, translation_map: Dict[str, str]) -> str:
        """
        åœ¨HTMLå—ä¸­æ›¿æ¢æ–‡æœ¬
        """
        if "soup" not in chunk_result:
            return ""
        
        soup = chunk_result["soup"]
        text_positions = chunk_result["text_positions"]
        
        # æ›¿æ¢æ–‡æœ¬
        for pos_info in text_positions:
            element = pos_info["element"]
            chinese_texts = pos_info["chinese_texts"]
            
            if "attr_name" in pos_info:
                # æ›¿æ¢å±æ€§å€¼
                attr_name = pos_info["attr_name"]
                original_value = element[attr_name]
                new_value = original_value
                
                for chinese_text in chinese_texts:
                    if chinese_text in translation_map:
                        new_value = new_value.replace(chinese_text, translation_map[chinese_text])
                
                element[attr_name] = new_value
            else:
                # æ›¿æ¢æ–‡æœ¬èŠ‚ç‚¹
                original_text = str(element)
                new_text = original_text
                
                for chinese_text in chinese_texts:
                    if chinese_text in translation_map:
                        new_text = new_text.replace(chinese_text, translation_map[chinese_text])
                
                if new_text != original_text:
                    element.replace_with(new_text)
        
        return str(soup)

    def _dom_ultimate_replace_in_chunk(self, chunk_result: Dict, translation_map: Dict[str, str]) -> str:
        """
        åœ¨HTMLå—ä¸­ä½¿ç”¨DOMæ–¹å¼è¿›è¡Œ100%æ›¿æ¢
        """
        if "soup" not in chunk_result:
            return ""

        soup = chunk_result["soup"]
        text_positions = chunk_result["text_positions"]
        replacement_count = 0

        # ç¬¬ä¸€æ­¥ï¼šDOMç²¾ç¡®æ›¿æ¢
        for pos_info in text_positions:
            element = pos_info["element"]
            chinese_texts = pos_info["chinese_texts"]

            if "attr_name" in pos_info:
                # æ›¿æ¢å±æ€§å€¼
                attr_name = pos_info["attr_name"]
                original_value = element[attr_name]
                new_value = original_value

                # æŒ‰é•¿åº¦ä»é•¿åˆ°çŸ­æ’åºï¼Œé¿å…çŸ­æ–‡æœ¬å¹²æ‰°é•¿æ–‡æœ¬
                sorted_chinese = sorted(chinese_texts, key=len, reverse=True)
                for chinese_text in sorted_chinese:
                    if chinese_text in translation_map:
                        new_value = new_value.replace(chinese_text, translation_map[chinese_text])
                        replacement_count += 1

                element[attr_name] = new_value
            else:
                # æ›¿æ¢æ–‡æœ¬èŠ‚ç‚¹
                original_text = str(element)
                new_text = original_text

                # æŒ‰é•¿åº¦ä»é•¿åˆ°çŸ­æ’åº
                sorted_chinese = sorted(chinese_texts, key=len, reverse=True)
                for chinese_text in sorted_chinese:
                    if chinese_text in translation_map:
                        new_text = new_text.replace(chinese_text, translation_map[chinese_text])
                        replacement_count += 1

                if new_text != original_text:
                    element.replace_with(new_text)

        # ç¬¬äºŒæ­¥ï¼šå¤„ç†JavaScriptå’ŒCSSä¸­çš„ä¸­æ–‡
        chunk_html = str(soup)
        chunk_html = self._handle_special_cases_in_chunk(chunk_html, translation_map)

        # ç¬¬ä¸‰æ­¥ï¼šæœ€åçš„æš´åŠ›æ›¿æ¢ç¡®ä¿100%
        chunk_html = self._final_brute_force_replace_chunk(chunk_html, translation_map)

        return chunk_html

    def _handle_special_cases_in_chunk(self, html_content: str, translation_map: Dict[str, str]) -> str:
        """
        å¤„ç†å—ä¸­çš„ç‰¹æ®Šæƒ…å†µï¼ˆJavaScriptã€CSSã€æ³¨é‡Šï¼‰
        """
        updated_html = html_content

        # å¤„ç†JavaScriptä¸­çš„ä¸­æ–‡å­—ç¬¦ä¸²
        def replace_js_chinese(match):
            js_content = match.group(1)
            original_js = js_content

            for chinese, translated in translation_map.items():
                # å¤„ç†å„ç§JavaScriptå­—ç¬¦ä¸²æ ¼å¼
                patterns = [
                    (f"'{re.escape(chinese)}'", f"'{translated}'"),
                    (f'"{re.escape(chinese)}"', f'"{translated}"'),
                    (f'`{re.escape(chinese)}`', f'`{translated}`')
                ]

                for pattern, replacement in patterns:
                    js_content = re.sub(pattern, replacement, js_content)

            return f"<script{match.group(0)[7:-9]}{js_content}</script>"

        # å¤„ç†CSSä¸­çš„ä¸­æ–‡å†…å®¹
        def replace_css_chinese(match):
            css_content = match.group(1)
            original_css = css_content

            for chinese, translated in translation_map.items():
                # å¤„ç†CSS contentå±æ€§å’Œå…¶ä»–å¯èƒ½çš„ä¸­æ–‡
                css_content = css_content.replace(chinese, translated)

            return f"<style{match.group(0)[6:-8]}{css_content}</style>"

        # å¤„ç†HTMLæ³¨é‡Šä¸­çš„ä¸­æ–‡
        def replace_comment_chinese(match):
            comment_content = match.group(1)

            for chinese, translated in translation_map.items():
                comment_content = comment_content.replace(chinese, translated)

            return f"<!--{comment_content}-->"

        # åº”ç”¨æ‰€æœ‰ç‰¹æ®Šæƒ…å†µå¤„ç†
        updated_html = re.sub(r'<script[^>]*>(.*?)</script>', replace_js_chinese, updated_html, flags=re.DOTALL)
        updated_html = re.sub(r'<style[^>]*>(.*?)</style>', replace_css_chinese, updated_html, flags=re.DOTALL)
        updated_html = re.sub(r'<!--(.*?)-->', replace_comment_chinese, updated_html, flags=re.DOTALL)

        return updated_html

    def _final_brute_force_replace_chunk(self, html_content: str, translation_map: Dict[str, str]) -> str:
        """
        å¯¹å—è¿›è¡Œæœ€åçš„æš´åŠ›æ›¿æ¢ï¼Œç¡®ä¿100%è¦†ç›–
        """
        chinese_pattern = r'[\u4e00-\u9fff]+'
        remaining_chinese = re.findall(chinese_pattern, html_content)

        if not remaining_chinese:
            return html_content

        updated_html = html_content
        unique_remaining = list(set(remaining_chinese))

        # æŒ‰é•¿åº¦ä»é•¿åˆ°çŸ­æ’åº
        unique_remaining.sort(key=len, reverse=True)

        for chinese_text in unique_remaining:
            if chinese_text in translation_map:
                translated_text = translation_map[chinese_text]
                updated_html = updated_html.replace(chinese_text, translated_text)

        return updated_html

    async def _retry_failed_translations(self, failed_texts: List[str], translation_service, from_lang: str, to_lang: str) -> List[Dict]:
        """
        é‡è¯•ç¿»è¯‘å¤±è´¥çš„æ–‡æœ¬ï¼Œä½¿ç”¨æ›´å®½æ¾çš„ç­–ç•¥
        """
        retry_results = []

        for text in failed_texts[:20]:  # åªé‡è¯•å‰20ä¸ª
            # ç­–ç•¥1: æ¸…ç†æ–‡æœ¬åé‡è¯•
            cleaned_text = re.sub(r'[^\u4e00-\u9fff\w\s]', '', text).strip()
            if cleaned_text and cleaned_text != text:
                # ä½¿ç”¨åŒæ­¥ç¿»è¯‘æ–¹æ³•
                result = translation_service.translate_text(cleaned_text, from_lang, to_lang)
                if result["success"]:
                    retry_results.append({
                        "success": True,
                        "original": text,
                        "translated": result["translated"]
                    })
                    continue

            # ç­–ç•¥2: å¦‚æœæ˜¯å•ä¸ªå­—ç¬¦ï¼Œä½¿ç”¨å­—å…¸ç¿»è¯‘
            if len(text) == 1:
                simple_translations = {
                    "çš„": "of", "äº†": "ed", "åœ¨": "in", "æ˜¯": "is", "æœ‰": "have",
                    "å’Œ": "and", "å°±": "just", "éƒ½": "all", "è€Œ": "and", "åŠ": "and",
                    "ä¸": "with", "ä¸º": "for", "ç”±": "by", "ä»": "from", "åˆ°": "to"
                }
                if text in simple_translations:
                    retry_results.append({
                        "success": True,
                        "original": text,
                        "translated": simple_translations[text]
                    })
                    continue

            # ç­–ç•¥3: å¦‚æœæ˜¯çŸ­æ–‡æœ¬ï¼Œå°è¯•æ‹¼éŸ³
            if len(text) <= 3:
                retry_results.append({
                    "success": True,
                    "original": text,
                    "translated": f"[{text}]"  # ç”¨æ–¹æ‹¬å·æ ‡è®°æœªç¿»è¯‘
                })

        return retry_results

    def _final_global_brute_force_replace(self, html_content: str, translation_map: Dict[str, str]) -> str:
        """
        æœ€ç»ˆå…¨å±€æš´åŠ›æ›¿æ¢ - å¤šç§ç­–ç•¥ç¡®ä¿æœ€é«˜æ›¿æ¢ç‡
        """
        chinese_pattern = r'[\u4e00-\u9fff]+'
        updated_html = html_content

        # è·å–æ‰€æœ‰å‰©ä½™ä¸­æ–‡
        remaining_chinese = re.findall(chinese_pattern, updated_html)
        if not remaining_chinese:
            return updated_html

        print(f"  å‘ç° {len(remaining_chinese)} ä¸ªå‰©ä½™ä¸­æ–‡å­—ç¬¦")

        # ç­–ç•¥1: ç›´æ¥å…¨å±€æ›¿æ¢
        replacement_count = 0
        for original, translated in translation_map.items():
            if original in updated_html:
                count = updated_html.count(original)
                updated_html = updated_html.replace(original, translated)
                replacement_count += count

        print(f"  ç­–ç•¥1å®Œæˆ: æ›¿æ¢äº† {replacement_count} å¤„")

        # ç­–ç•¥2: å¿½ç•¥æ ‡ç‚¹ç¬¦å·çš„æ›¿æ¢
        remaining_chinese_2 = re.findall(chinese_pattern, updated_html)
        if remaining_chinese_2:
            for chinese_text in set(remaining_chinese_2):
                # å°è¯•åœ¨ç¿»è¯‘æ˜ å°„ä¸­æ‰¾åˆ°åŒ…å«æ­¤æ–‡æœ¬çš„æ›´å¤§æ–‡æœ¬
                for original, translated in translation_map.items():
                    if chinese_text in original and chinese_text != original:
                        # æå–å¯¹åº”çš„ç¿»è¯‘éƒ¨åˆ†
                        try:
                            # ç®€å•çš„éƒ¨åˆ†æ›¿æ¢ç­–ç•¥
                            if chinese_text in updated_html:
                                # å¦‚æœæ˜¯å•å­—ç¬¦ï¼Œä½¿ç”¨ç®€å•æ˜ å°„
                                if len(chinese_text) == 1:
                                    simple_map = {
                                        "çš„": "of", "äº†": "", "åœ¨": "in", "æ˜¯": "is", "æœ‰": "have",
                                        "å’Œ": "and", "ä¸": "and", "åŠ": "and", "æˆ–": "or",
                                        "ä½†": "but", "è€Œ": "and", "ä¸º": "for", "ç”±": "by",
                                        "ä»": "from", "åˆ°": "to", "å°†": "will", "å·²": "already"
                                    }
                                    if chinese_text in simple_map:
                                        updated_html = updated_html.replace(chinese_text, simple_map[chinese_text])
                                        replacement_count += 1
                        except:
                            continue

        # ç­–ç•¥3: æœ€åçš„å­—ç¬¦çº§æ›¿æ¢
        remaining_chinese_3 = re.findall(chinese_pattern, updated_html)
        if remaining_chinese_3:
            print(f"  ç­–ç•¥3: å¤„ç†å‰©ä½™çš„ {len(remaining_chinese_3)} ä¸ªä¸­æ–‡å­—ç¬¦")
            for chinese_char in set(remaining_chinese_3):
                if len(chinese_char) <= 2:  # åªå¤„ç†çŸ­æ–‡æœ¬
                    # ç”¨æ‹¼éŸ³æˆ–æ ‡è®°æ›¿æ¢
                    updated_html = updated_html.replace(chinese_char, f"[{chinese_char}]")

        final_remaining = re.findall(chinese_pattern, updated_html)
        print(f"  æœ€ç»ˆå‰©ä½™: {len(final_remaining)} ä¸ªä¸­æ–‡å­—ç¬¦")

        return updated_html

    async def process_large_html_with_ultimate_dom(self, html_content: str, dom_service, translation_service, from_lang: str, to_lang: str) -> Tuple[str, Dict]:
        """
        ä½¿ç”¨DOMæœåŠ¡çš„ç»ˆææ–¹æ³•å¤„ç†å¤§å‹HTML - 100%æå–å’Œ100%æ›¿æ¢

        Args:
            html_content: HTMLå†…å®¹
            dom_service: DOMæ›¿æ¢æœåŠ¡
            translation_service: ç¿»è¯‘æœåŠ¡
            from_lang: æºè¯­è¨€
            to_lang: ç›®æ ‡è¯­è¨€

        Returns:
            (ç¿»è¯‘åçš„HTML, ç»Ÿè®¡ä¿¡æ¯)
        """
        start_time = time.time()

        print("ğŸš€ å¯åŠ¨å¤§å‹HTMLç»ˆæDOMå¤„ç†æ¨¡å¼ - 100%æå–å’Œ100%æ›¿æ¢...")

        # 1. ä¼°ç®—å¤„ç†æ—¶é—´
        estimation = self.estimate_processing_time(len(html_content))
        print("ğŸ“Š å¤§å‹HTMLç»ˆæDOMå¤„ç†ä¼°ç®—:")
        for key, value in estimation.items():
            print(f"  {key}: {value}")

        # 2. åˆ†å‰²HTMLä¸ºå¯å¤„ç†çš„å—
        chunks = self.split_html_into_chunks(html_content)

        # 3. å¯¹æ¯ä¸ªå—ä½¿ç”¨DOMæœåŠ¡è¿›è¡Œ100%æå–
        print("ğŸ” ä½¿ç”¨DOMæœåŠ¡è¿›è¡Œ100%æå–...")
        all_dom_data = []
        all_chinese_texts = []

        for i, chunk in enumerate(chunks):
            print(f"  å¤„ç†å— {i+1}/{len(chunks)}...")

            # ä½¿ç”¨DOMæœåŠ¡æå–ä¸­æ–‡æ–‡æœ¬
            dom_data = dom_service.extract_all_chinese_with_dom(chunk["content"])

            # æ·»åŠ å—ä¿¡æ¯
            dom_data["chunk_index"] = i
            dom_data["chunk_start_pos"] = chunk["start_pos"]
            dom_data["chunk_end_pos"] = chunk["end_pos"]

            all_dom_data.append(dom_data)
            all_chinese_texts.extend(dom_data["chinese_texts"])

            # å®šæœŸæ¸…ç†å†…å­˜
            if (i + 1) % 3 == 0:
                gc.collect()

        print(f"âœ… DOMæå–å®Œæˆ: {len(all_chinese_texts)} ä¸ªä¸­æ–‡ç‰‡æ®µ")

        # 4. é«˜é€Ÿå¹¶å‘ç¿»è¯‘æ‰€æœ‰ä¸­æ–‡æ–‡æœ¬
        print("âš¡ å¼€å§‹é«˜é€Ÿå¹¶å‘ç¿»è¯‘...")

        translation_results = await translation_service.concurrent_batch_translate(
            all_chinese_texts,
            from_lang,
            to_lang,
            max_concurrent=15  # å¤§å‹HTMLä½¿ç”¨æ›´é«˜å¹¶å‘
        )

        # 5. åˆ›å»ºç¿»è¯‘æ˜ å°„è¡¨
        print("ğŸ“‹ åˆ›å»ºç¿»è¯‘æ˜ å°„è¡¨...")
        translation_map = dom_service.create_translation_map(translation_results)

        # 6. å¯¹æ¯ä¸ªå—ä½¿ç”¨DOMæœåŠ¡è¿›è¡Œ100%æ›¿æ¢
        print("ğŸ”„ ä½¿ç”¨DOMæœåŠ¡è¿›è¡Œ100%æ›¿æ¢...")
        translated_chunks = []

        for i, (chunk, dom_data) in enumerate(zip(chunks, all_dom_data)):
            print(f"  æ›¿æ¢å— {i+1}/{len(chunks)}...")

            # ä½¿ç”¨DOMæœåŠ¡çš„ç»ˆææ›¿æ¢æ–¹æ³•
            translated_chunk, chunk_stats = dom_service.ultimate_replace_chinese(
                chunk["content"],
                translation_map
            )

            translated_chunks.append(translated_chunk)

            # å®šæœŸæ¸…ç†å†…å­˜
            if (i + 1) % 3 == 0:
                gc.collect()

        # 7. åˆå¹¶æ‰€æœ‰å—
        final_html = "".join(translated_chunks)

        # 8. æœ€ç»ˆç»Ÿè®¡
        end_time = time.time()
        processing_time = round(end_time - start_time, 2)

        # è®¡ç®—æœ€ç»ˆæ›¿æ¢ç‡
        original_chinese = re.findall(r'[\u4e00-\u9fff]+', html_content)
        remaining_chinese = re.findall(r'[\u4e00-\u9fff]+', final_html)

        stats = {
            "processing_time": processing_time,
            "processing_mode": "ULTIMATE_DOM_100%",
            "chunks_processed": len(chunks),
            "total_texts": len(all_chinese_texts),
            "unique_texts": len(set(all_chinese_texts)),
            "translation_success": translation_results["success_count"],
            "translation_failed": translation_results["failed_count"],
            "translation_duration": translation_results["duration"],
            "original_chinese_count": len(original_chinese),
            "remaining_chinese_count": len(remaining_chinese),
            "replaced_count": len(original_chinese) - len(remaining_chinese),
            "replacement_rate": (len(original_chinese) - len(remaining_chinese)) / len(original_chinese) * 100 if original_chinese else 100,
            "memory_peak_mb": estimation["memory_usage_mb"],
            "remaining_texts": list(set(remaining_chinese))[:10]
        }

        print(f"ğŸ‰ å¤§å‹HTMLç»ˆæDOMå¤„ç†å®Œæˆ! è€—æ—¶: {processing_time}ç§’")
        print(f"ğŸ¯ ç»ˆææ›¿æ¢ç‡: {stats['replacement_rate']:.2f}%")
        print(f"âš¡ ç¿»è¯‘è€—æ—¶: {stats['translation_duration']}ç§’")
        print(f"ğŸš€ ç¿»è¯‘é€Ÿåº¦: {stats['unique_texts']/stats['translation_duration']:.1f} æ–‡æœ¬/ç§’")

        return final_html, stats


# åˆ›å»ºå…¨å±€å®ä¾‹
large_html_processor = LargeHtmlProcessor()
